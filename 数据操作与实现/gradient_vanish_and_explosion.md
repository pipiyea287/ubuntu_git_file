### gradient explosion
using  ReLU as the activation function. Considering that in backward propagation, there will be so much gradient multiplying by themselves.   
If the graident is 1.5, after multiplying its by 100 times, it will be a so big number.
### gradient vanish
Using sigmoid as the activation function. The same as the talking above, the gradient generated by sigmoid is not so large. In instance, it is just 0.8,   
and after multiplying itself 100 times, it is almost 0. Then  no matter how you adjust the lr(learning rate), the loss will not decrease.
### summary
choosing proper initial value of the weight and type of the activation function will increase the numerical stablity.
